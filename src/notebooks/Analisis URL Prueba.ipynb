{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.twitch.tv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vamos a crear una función que, mediante diferentes funciones, extraiga todos los parámetros que necesitamos en el orden de las columnas\n",
    "del dataframe del que hemos 'enseñado' al modelo para realizar la predicción.\n",
    "\n",
    "En primer lugar, vamos a importar todas las bibliotecas necesarias:\n",
    "\n",
    "'''\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import whois\n",
    "import time\n",
    "import re\n",
    "import urllib\n",
    "import tldextract\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "'''\n",
    "Lo primero que vamos a tener que comprobar es si la url es accesible o no.\n",
    "\n",
    "'''\n",
    "def accesibilidad_URL(url):\n",
    "    pagina = None\n",
    "    try:\n",
    "        pagina = requests.get(url, timeout=5) # Intenta acceder a la URL proporcionada a través de requests y asigna la respuesta a 'page'\n",
    "    except:\n",
    "        parseo = urlparse(url)\n",
    "        url = parseo.scheme+'://'+parseo.netloc # Crea una nueva URL sin \"www\"\n",
    "        if not parseo.netloc.startswith('www'): \n",
    "            url = parseo.scheme+'://www.'+parseo.netloc\n",
    "            try:\n",
    "                pagina = requests.get(url, timeout=5) # Intenta acceder a la nueva URL creada a través de requests\n",
    "            except:\n",
    "                pagina = None\n",
    "                pass\n",
    "    if pagina and pagina.status_code == 200 and pagina.content not in [\"b''\", \"b' '\"]: #Si accede y el contenido no es una cadena vacía\n",
    "        return True, url, pagina\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "'''\n",
    "Lo siguiente que vamos a hacer, es extraer el dominio de la URL y guardarlo en una variable, lo utilizaremos más adelante.\n",
    "\n",
    "'''\n",
    "def sacar_dominio(url):\n",
    "    o = urllib.parse.urlsplit(url)\n",
    "    return o.hostname, tldextract.extract(url).domain, o.path # Devuelve una tupla que contiene el nombre de host, el dominio y la ruta\n",
    "\n",
    "'''\n",
    "Lo siguiente será extraer todos los datos posibles de la página web mediante escrapeo.\n",
    "\n",
    "Esta función esta extraida de la documentación de los creadores del dataframe original para la extracción de los datos necesarios.\n",
    "Con esta función crearemos una serie de diccionarios con diferentes elementos que serán necesarios para las funciones que crearemos después.\n",
    "\n",
    "'''\n",
    "def extraccion_datos_URL(hostname, content, domain, Href, Link, Anchor, Media, Form, CSS, Favicon, IFrame, Title, Text):\n",
    "    Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
    "\n",
    "    # Recolectar todos los href externos e internos de la URL\n",
    "    for href in soup.find_all('a', href=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', href['href'])]\n",
    "        if hostname in href['href'] or domain in href['href'] or len(dots) == 1 or not href['href'].startswith('http'):\n",
    "            if \"#\" in href['href'] or \"javascript\" in href['href'].lower() or \"mailto\" in href['href'].lower():\n",
    "                 Anchor['unsafe'].append(href['href']) \n",
    "            if not href['href'].startswith('http'):\n",
    "                if not href['href'].startswith('/'):\n",
    "                    Href['internals'].append(hostname+'/'+href['href']) \n",
    "                elif href['href'] in Null_format:\n",
    "                    Href['null'].append(href['href'])  \n",
    "                else:\n",
    "                    Href['internals'].append(hostname+href['href'])   \n",
    "        else:\n",
    "            Href['externals'].append(href['href'])\n",
    "            Anchor['safe'].append(href['href'])\n",
    "\n",
    "    # Recolectar todas las etiquetas de src de los medios\n",
    "    for img in soup.find_all('img', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
    "        if hostname in img['src'] or domain in img['src'] or len(dots) == 1 or not img['src'].startswith('http'):\n",
    "            if not img['src'].startswith('http'):\n",
    "                if not img['src'].startswith('/'):\n",
    "                    Media['internals'].append(hostname+'/'+img['src']) \n",
    "                elif img['src'] in Null_format:\n",
    "                    Media['null'].append(img['src'])  \n",
    "                else:\n",
    "                    Media['internals'].append(hostname+img['src'])   \n",
    "        else:\n",
    "            Media['externals'].append(img['src'])\n",
    "           \n",
    "    \n",
    "    for audio in soup.find_all('audio', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
    "        if hostname in audio['src'] or domain in audio['src'] or len(dots) == 1 or not audio['src'].startswith('http'):\n",
    "             if not audio['src'].startswith('http'):\n",
    "                if not audio['src'].startswith('/'):\n",
    "                    Media['internals'].append(hostname+'/'+audio['src']) \n",
    "                elif audio['src'] in Null_format:\n",
    "                    Media['null'].append(audio['src'])  \n",
    "                else:\n",
    "                    Media['internals'].append(hostname+audio['src'])   \n",
    "        else:\n",
    "            Media['externals'].append(audio['src'])\n",
    "            \n",
    "    for embed in soup.find_all('embed', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', embed['src'])]\n",
    "        if hostname in embed['src'] or domain in embed['src'] or len(dots) == 1 or not embed['src'].startswith('http'):\n",
    "             if not embed['src'].startswith('http'):\n",
    "                if not embed['src'].startswith('/'):\n",
    "                    Media['internals'].append(hostname+'/'+embed['src']) \n",
    "                elif embed['src'] in Null_format:\n",
    "                    Media['null'].append(embed['src'])  \n",
    "                else:\n",
    "                    Media['internals'].append(hostname+embed['src'])   \n",
    "        else:\n",
    "            Media['externals'].append(embed['src'])\n",
    "           \n",
    "    for i_frame in soup.find_all('iframe', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', i_frame['src'])]\n",
    "        if hostname in i_frame['src'] or domain in i_frame['src'] or len(dots) == 1 or not i_frame['src'].startswith('http'):\n",
    "            if not i_frame['src'].startswith('http'):\n",
    "                if not i_frame['src'].startswith('/'):\n",
    "                    Media['internals'].append(hostname+'/'+i_frame['src']) \n",
    "                elif i_frame['src'] in Null_format:\n",
    "                    Media['null'].append(i_frame['src'])  \n",
    "                else:\n",
    "                    Media['internals'].append(hostname+i_frame['src'])   \n",
    "        else: \n",
    "            Media['externals'].append(i_frame['src'])\n",
    "           \n",
    "\n",
    "    # Recopilar todos las etiquetas links\n",
    "    for link in soup.findAll('link', href=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', link['href'])]\n",
    "        if hostname in link['href'] or domain in link['href'] or len(dots) == 1 or not link['href'].startswith('http'):\n",
    "            if not link['href'].startswith('http'):\n",
    "                if not link['href'].startswith('/'):\n",
    "                    Link['internals'].append(hostname+'/'+link['href']) \n",
    "                elif link['href'] in Null_format:\n",
    "                    Link['null'].append(link['href'])  \n",
    "                else:\n",
    "                    Link['internals'].append(hostname+link['href'])   \n",
    "        else:\n",
    "            Link['externals'].append(link['href'])\n",
    "\n",
    "    for script in soup.find_all('script', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', script['src'])]\n",
    "        if hostname in script['src'] or domain in script['src'] or len(dots) == 1 or not script['src'].startswith('http'):\n",
    "            if not script['src'].startswith('http'):\n",
    "                if not script['src'].startswith('/'):\n",
    "                    Link['internals'].append(hostname+'/'+script['src']) \n",
    "                elif script['src'] in Null_format:\n",
    "                    Link['null'].append(script['src'])  \n",
    "                else:\n",
    "                    Link['internals'].append(hostname+script['src'])   \n",
    "        else:\n",
    "            Link['externals'].append(link['href'])\n",
    "           \n",
    "            \n",
    "    # Recopilar todos las etiquetas CSS\n",
    "    for link in soup.find_all('link', rel='stylesheet'):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', link['href'])]\n",
    "        if hostname in link['href'] or domain in link['href'] or len(dots) == 1 or not link['href'].startswith('http'):\n",
    "            if not link['href'].startswith('http'):\n",
    "                if not link['href'].startswith('/'):\n",
    "                    CSS['internals'].append(hostname+'/'+link['href']) \n",
    "                elif link['href'] in Null_format:\n",
    "                    CSS['null'].append(link['href'])  \n",
    "                else:\n",
    "                    CSS['internals'].append(hostname+link['href'])   \n",
    "        else:\n",
    "            CSS['externals'].append(link['href'])\n",
    "    \n",
    "    for style in soup.find_all('style', type='text/css'):\n",
    "        try: \n",
    "            start = str(style[0]).index('@import url(')\n",
    "            end = str(style[0]).index(')')\n",
    "            css = str(style[0])[start+12:end]\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', css)]\n",
    "            if hostname in css or domain in css or len(dots) == 1 or not css.startswith('http'):\n",
    "                if not css.startswith('http'):\n",
    "                    if not css.startswith('/'):\n",
    "                        CSS['internals'].append(hostname+'/'+css) \n",
    "                    elif css in Null_format:\n",
    "                        CSS['null'].append(css)  \n",
    "                    else:\n",
    "                        CSS['internals'].append(hostname+css)   \n",
    "            else: \n",
    "                CSS['externals'].append(css)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Recopilar todos las acciones Form\n",
    "    for form in soup.findAll('form', action=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', form['action'])]\n",
    "        if hostname in form['action'] or domain in form['action'] or len(dots) == 1 or not form['action'].startswith('http'):\n",
    "            if not form['action'].startswith('http'):\n",
    "                if not form['action'].startswith('/'):\n",
    "                    Form['internals'].append(hostname+'/'+form['action']) \n",
    "                elif form['action'] in Null_format or form['action'] == 'about:blank':\n",
    "                    Form['null'].append(form['action'])  \n",
    "                else:\n",
    "                    Form['internals'].append(hostname+form['action'])   \n",
    "        else:\n",
    "            Form['externals'].append(form['action'])\n",
    "            \n",
    "\n",
    "    # Recopilar todos loas etiquetas link\n",
    "    for head in soup.find_all('head'):\n",
    "        for head.link in soup.find_all('link', href=True):\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
    "            if hostname in head.link['href'] or len(dots) == 1 or domain in head.link['href'] or not head.link['href'].startswith('http'):\n",
    "                if not head.link['href'].startswith('http'):\n",
    "                    if not head.link['href'].startswith('/'):\n",
    "                        Favicon['internals'].append(hostname+'/'+head.link['href']) \n",
    "                    elif head.link['href'] in Null_format:\n",
    "                        Favicon['null'].append(head.link['href'])  \n",
    "                    else:\n",
    "                        Favicon['internals'].append(hostname+head.link['href'])   \n",
    "            else:\n",
    "                Favicon['externals'].append(head.link['href'])\n",
    "                \n",
    "        for head.link in soup.findAll('link', {'href': True, 'rel':True}):\n",
    "            isicon = False\n",
    "            if isinstance(head.link['rel'], list):\n",
    "                for e_rel in head.link['rel']:\n",
    "                    if (e_rel.endswith('icon')):\n",
    "                        isicon = True\n",
    "            else:\n",
    "                if (head.link['rel'].endswith('icon')):\n",
    "                    isicon = True\n",
    "       \n",
    "            if isicon:\n",
    "                 dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
    "                 if hostname in head.link['href'] or len(dots) == 1 or domain in head.link['href'] or not head.link['href'].startswith('http'):\n",
    "                     if not head.link['href'].startswith('http'):\n",
    "                        if not head.link['href'].startswith('/'):\n",
    "                            Favicon['internals'].append(hostname+'/'+head.link['href']) \n",
    "                        elif head.link['href'] in Null_format:\n",
    "                            Favicon['null'].append(head.link['href'])  \n",
    "                        else:\n",
    "                            Favicon['internals'].append(hostname+head.link['href'])   \n",
    "                 else:\n",
    "                     Favicon['externals'].append(head.link['href'])\n",
    "                     \n",
    "                    \n",
    "    # Recopilar todas las iFrame\n",
    "    for i_frame in soup.find_all('iframe', width=True, height=True, frameborder=True):\n",
    "        if i_frame['width'] == \"0\" and i_frame['height'] == \"0\" and i_frame['frameborder'] == \"0\":\n",
    "            IFrame['invisible'].append(i_frame)\n",
    "        else:\n",
    "            IFrame['visible'].append(i_frame)\n",
    "    for i_frame in soup.find_all('iframe', width=True, height=True, border=True):\n",
    "        if i_frame['width'] == \"0\" and i_frame['height'] == \"0\" and i_frame['border'] == \"0\":\n",
    "            IFrame['invisible'].append(i_frame)\n",
    "        else:\n",
    "            IFrame['visible'].append(i_frame)\n",
    "    for i_frame in soup.find_all('iframe', width=True, height=True, style=True):\n",
    "        if i_frame['width'] == \"0\" and i_frame['height'] == \"0\" and i_frame['style'] == \"border:none;\":\n",
    "            IFrame['invisible'].append(i_frame)\n",
    "        else:\n",
    "            IFrame['visible'].append(i_frame)\n",
    "          \n",
    "    # Titulo de la página\n",
    "    try:\n",
    "        Title = soup.title.string\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Recoger contenido del texto\n",
    "    Text = soup.get_text()\n",
    "    \n",
    "    return Href, Link, Anchor, Media, Form, CSS, Favicon, IFrame, Title, Text\n",
    "\n",
    "\n",
    "'''\n",
    "Ahora toca definir las funciones según el orden de las columnas de nuestro dataframe.\n",
    "\n",
    "Vamos a empezar con una función que nos indique si la página web aparece en el índice de Google, (google_index).\n",
    "\n",
    "'''\n",
    "def google_index(url):\n",
    "\n",
    "    servicio = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=servicio)\n",
    "\n",
    "    google = 'https://www.google.com/webhp?hl=es&sa=X&ved=0ahUKEwjR1qmah6f-AhU8VqQEHQKCCAcQPAgJ'\n",
    "    driver.get(google) # Escrapeo a google para ver si existe la pagina en google  \n",
    "\n",
    "    aceptar = driver.find_element(By.ID, \"L2AGLb\")\n",
    "    aceptar.click()\n",
    "    time.sleep(5)\n",
    "    buscar = driver.find_element(By.ID, 'APjFqb')\n",
    "    buscar.send_keys(f'site:{url}')\n",
    "    time.sleep(5)\n",
    "    buscar.send_keys(Keys.ENTER)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        url_1 = driver.find_element(By.CLASS_NAME, \"qLRx3b.tjvcx.GvPZzd.cHaqb\")\n",
    "        url_text = url_1.text\n",
    "    except:\n",
    "        url_text = '0'\n",
    "    url_text = url_text[:(len(url) + 1)]\n",
    "    if url == url_text:\n",
    "        google_index = 0\n",
    "    else:\n",
    "        google_index = 1\n",
    "    return google_index\n",
    "\n",
    "\n",
    "'''\n",
    "'Page_rank': Ranking de la página web.\n",
    "\n",
    "'''\n",
    "def page_rank(dominio):\n",
    "    url = 'https://www.alexa.com/siteinfo/' + dominio # Escrapeo a la web de Alexa\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        rank = soup.find('div', {'class': 'rank-global'}).find('strong').get_text()\n",
    "        if rank:\n",
    "            return int(rank.replace(',', ''))\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "'''\n",
    "'Nb_hyperlinks': Número de hipervínculos en la página web.\n",
    "\n",
    "'''\n",
    "def nb_hyperlinks(Href, Link, Media, Form, CSS, Favicon): # Diccionarios con los tipos de enlaces internos y externos\n",
    "    return len(Href['internals']) + len(Href['externals']) +\\\n",
    "            len(Link['internals']) + len(Link['externals']) +\\\n",
    "            len(Media['internals']) + len(Media['externals']) +\\\n",
    "            len(Form['internals']) + len(Form['externals']) +\\\n",
    "            len(CSS['internals']) + len(CSS['externals']) +\\\n",
    "            len(Favicon['internals']) + len(Favicon['externals'])\n",
    "\n",
    "\n",
    "# Redirecciones totales (Nos servirán para otros parámetro próximos como los hipervinculos externos)\n",
    "\n",
    "def redirecciones_totales(Href, Link, Media, Form, CSS, Favicon):\n",
    "    return nb_hyperlinks(Href, Link, Media, Form, CSS, Favicon)\n",
    "\n",
    "'''\n",
    "'Traffic_web': Tráfico de la página web.\n",
    "\n",
    "'''\n",
    "def traffic_web(url):\n",
    "        try: #Escrapeo a la API de Alexa y devuelve el rango como un entero\n",
    "            rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK'] \n",
    "        except:\n",
    "            return 0\n",
    "        return int(rank)\n",
    "\n",
    "\n",
    "'''\n",
    "'Nb_www': Indica si la URL contiene \"www\" en el nombre de host.\n",
    "\n",
    "'''\n",
    "def check_www(words_raw):\n",
    "        count = 0\n",
    "        for word in words_raw:\n",
    "            if not word.find('www') == -1:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "\n",
    "'''\n",
    "'Domain_age': Antigüedad del dominio.\n",
    "\n",
    "'''\n",
    "\n",
    "def domain_age(dominio):\n",
    "    try:\n",
    "        w = whois.whois(dominio)\n",
    "        creation_date = w.creation_date\n",
    "        if type(creation_date) is list:\n",
    "            creation_date = creation_date[0]\n",
    "        age = (datetime.datetime.now() - creation_date).days\n",
    "        return age\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "'''\n",
    "'Ratio_extHyperlinks': Proporción de hipervínculos externos en la página web.\n",
    "\n",
    "'''\n",
    "# Aprovechando la función de las redirecciones totales hayamos las externas\n",
    "def redirecciones_externas(Href, Link, Media, Form, CSS, Favicon):\n",
    "    return len(Href['externals']) + len(Link['externals']) + len(Media['externals']) +\\\n",
    "           len(Form['externals']) + len(CSS['externals']) + len(Favicon['externals'])\n",
    "\n",
    "def external_hyperlinks(Href, Link, Media, Form, CSS, Favicon):\n",
    "    total = redirecciones_totales(Href, Link, Media, Form, CSS, Favicon)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    else :\n",
    "        redirecciones_externas(Href, Link, Media, Form, CSS, Favicon)/total\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "'Longest_word_path': Longitud de la palabra más larga en la ruta de la URL sin caracteres especiales.\n",
    "\n",
    "'''\n",
    "def longest_word_length(words_raw_path):\n",
    "    if len(words_raw_path) ==0:\n",
    "        return 0\n",
    "    return max(len(word) for word in words_raw_path)\n",
    "\n",
    "\n",
    "'''\n",
    "'Phish_hints': Indica si la URL contiene palabras clave asociadas con phishing.\n",
    "\n",
    "'''\n",
    "# Según la documentación del dataframe, estas son las palabras que se usaron para extraer estos datos\n",
    "HINTS = ['wp', 'login', 'includes', 'admin', 'content', 'site', 'images', 'js', 'alibaba', 'css', 'myaccount', 'dropbox', 'themes', 'plugins', 'signin', 'view']\n",
    "def phish_hints(url_path):\n",
    "    count = 0\n",
    "    for hint in HINTS:\n",
    "        count += url_path.lower().count(hint)\n",
    "    return count\n",
    "\n",
    "\n",
    "'''\n",
    "'Ratio_intHyperlinks': Proporción de hipervínculos internos en la página web.\n",
    "\n",
    "'''\n",
    "# Aprovechando la función de las redirecciones totales hayamos las externas\n",
    "def redirecciones_internas(Href, Link, Media, Form, CSS, Favicon):\n",
    "    return len(Href['internals']) + len(Link['internals']) + len(Media['internals']) +\\\n",
    "           len(Form['internals']) + len(CSS['internals']) + len(Favicon['internals'])\n",
    "\n",
    "def internal_hyperlinks(Href, Link, Media, Form, CSS, Favicon):\n",
    "    total = redirecciones_totales(Href, Link, Media, Form, CSS, Favicon)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    else :\n",
    "        return redirecciones_internas(Href, Link, Media, Form, CSS, Favicon)/total\n",
    "    \n",
    "\n",
    "'''\n",
    "'Safe_anchor': Porcentaje de enlaces inseguros en la página web.\n",
    "\n",
    "'''\n",
    "def safe_anchor(Anchor): # El diccionario Anchor contiene los elementos 'rel' seguros e inseguros\n",
    "    total = len(Anchor['safe']) +  len(Anchor['unsafe'])\n",
    "    unsafe = len(Anchor['unsafe'])\n",
    "    try:\n",
    "        percentile = unsafe / float(total) * 100\n",
    "    except:\n",
    "        return 0\n",
    "    return percentile \n",
    "\n",
    "\n",
    "'''\n",
    "'Ratio_digits_url': Proporción de dígitos en la URL.\n",
    "\n",
    "'''\n",
    "def count_digits(line):\n",
    "    return len(re.sub(\"[^0-9]\", \"\", line)) # La expresión regular con la función '.sub' sustituye los caracteres que no son números por un string vacío\n",
    "\n",
    "\n",
    "'''\n",
    "'Longest_words_raw': Longitud de la palabra más larga en la URL sin caracteres especiales.\n",
    "\n",
    "'''\n",
    "def longest_word_length(words_raw):\n",
    "    if len(words_raw) ==0:\n",
    "        return 0\n",
    "    return max(len(word) for word in words_raw) \n",
    "\n",
    "\n",
    "'''\n",
    "'Length_hostname': Longitud del nombre de host en la URL.\n",
    "\n",
    "'''\n",
    "def url_length(hostname):\n",
    "    return len(hostname) \n",
    "\n",
    "\n",
    "'''\n",
    "'Links_in_tags': Indica si hay enlaces en las etiquetas HTML de la página web.\n",
    "\n",
    "'''\n",
    "def links_in_tags(Link): # El diccionario Link contiene los links internos y externos\n",
    "    total = len(Link['internals']) +  len(Link['externals'])\n",
    "    internals = len(Link['internals'])\n",
    "    try:\n",
    "        percentile = internals / float(total) * 100\n",
    "    except:\n",
    "        return 0\n",
    "    return percentile\n",
    "\n",
    "\n",
    "'''\n",
    "'Avg_word_path': Longitud promedio de las palabras en la ruta de la URL sin caracteres especiales.\n",
    "\n",
    "'''\n",
    "def average_word_length(words_raw_path):\n",
    "    if len(words_raw_path) ==0:\n",
    "        return 0\n",
    "    return sum(len(word) for word in words_raw_path) / len(words_raw_path)\n",
    "\n",
    "\n",
    "'''\n",
    "'Length_words_raw': Longitud de la URL sin caracteres especiales.\n",
    "\n",
    "'''\n",
    "def length_word_raw(words_raw):\n",
    "    return len(words_raw)\n",
    "\n",
    "\n",
    "'''\n",
    "'Char_repeat': Indica si hay caracteres repetidos en la URL.\n",
    "\n",
    "'''\n",
    "def char_repeat(words_raw):\n",
    "    def iguales(items):\n",
    "        return all(x == items[0] for x in items) #mira si son todos los caracteres iguales\n",
    "    repeat = {'2': 0, '3': 0, '4': 0, '5': 0}\n",
    "    part = [2, 3, 4, 5]\n",
    "    for word in words_raw:\n",
    "        for char_repeat_count in part:\n",
    "            for i in range(len(word) - char_repeat_count + 1):\n",
    "                sub_word = word[i:i + char_repeat_count]\n",
    "                if iguales(sub_word):\n",
    "                    repeat[str(char_repeat_count)] = repeat[str(char_repeat_count)] + 1\n",
    "    return sum(list(repeat.values()))\n",
    "\n",
    "\n",
    "'''\n",
    "'Domain_registration_length': Longitud del registro del dominio.\n",
    "\n",
    "'''\n",
    "def domain_registration_length(dominio):\n",
    "    longitud_dominio = len(dominio)\n",
    "    return longitud_dominio\n",
    "\n",
    "\n",
    "'''\n",
    "'Ratio_extRedirection': Proporción de redirecciones externas en la página web.\n",
    "\n",
    "'''\n",
    "def num_enlacesExt(Href, Link, Media, Form, CSS, Favicon):\n",
    "    count = 0\n",
    "    for link in Href['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Link['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Media['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Media['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue \n",
    "    for link in Form['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue    \n",
    "    for link in CSS['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue    \n",
    "    for link in Favicon['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue    \n",
    "    return count\n",
    "\n",
    "def external_redirection(Href, Link, Media, Form, CSS, Favicon):\n",
    "    externals = redirecciones_externas(Href, Link, Media, Form, CSS, Favicon)\n",
    "    if (externals>0):\n",
    "        return num_enlacesExt(Href, Link, Media, Form, CSS, Favicon)/externals\n",
    "    return 0\n",
    "\n",
    "\n",
    "'''\n",
    "'Shortest_word_host': Longitud de la palabra más corta en el nombre de host de la URL sin caracteres especiales.\n",
    "\n",
    "'''\n",
    "def shortest_word_length(words_raw_host):\n",
    "    if len(words_raw_host) ==0:\n",
    "        return 0\n",
    "    return min(len(word) for word in words_raw_host)\n",
    "\n",
    "\n",
    "'''\n",
    "'Ratio_digits_host': Proporción de dígitos en el nombre de host de la URL.\n",
    "\n",
    "'''\n",
    "def ratio_digits(hostname):\n",
    "    return len(re.sub(\"[^0-9]\", \"\", hostname))/len(hostname)\n",
    "\n",
    "\n",
    "'''\n",
    "'Nb_slash': Número de barras diagonales en la URL.\n",
    "\n",
    "'''\n",
    "def count_slash(full_url):\n",
    "    return full_url.count('/')\n",
    "\n",
    "\n",
    "'''\n",
    "'Shortest_word_path': Longitud de la palabra más corta en la ruta de la URL sin caracteres especiales.\n",
    "\n",
    "'''\n",
    "def shortest_word_length(words_raw_path):\n",
    "    if len(words_raw_path) ==0:\n",
    "        return 0\n",
    "    return min(len(word) for word in words_raw_path)\n",
    "\n",
    "\n",
    "'''\n",
    "'Domain_in_title': Indica si el nombre de dominio está incluido en el título de la página web.\n",
    "\n",
    "'''\n",
    "def domain_in_title(domain, title):\n",
    "    if domain.lower() in title.lower(): \n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "'''\n",
    "'Nb_dots': Número de puntos en la URL.\n",
    "\n",
    "'''\n",
    "def count_dots(hostname):\n",
    "    return hostname.count('.')\n",
    "\n",
    "\n",
    "'''\n",
    "'Nb_hyphens': número de guiones en la URL.\n",
    "\n",
    "'''\n",
    "def count_hyphens(base_url):\n",
    "    return base_url.count('-')\n",
    "\n",
    "\n",
    "'''\n",
    "'Avg_words_raw': Longitud promedio de las palabras en la URL sin caracteres especiales.\n",
    "\n",
    "'''\n",
    "def average_word_length(words_raw):\n",
    "    if len(words_raw) ==0:\n",
    "        return 0\n",
    "    return sum(len(word) for word in words_raw) / len(words_raw)\n",
    "\n",
    "\n",
    "'''\n",
    "Esta será la función que utilizaremos para extraer todos los parámetros y convertirlos en una lista del mismo orden que las \n",
    "filas y columnas del dataframe de nuestro modelo.\n",
    "\n",
    "'''\n",
    "def extraccion_parametros(url):\n",
    "    \n",
    "    def extraccion_palabras(domain, subdomain, path):\n",
    "        w_domain = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", domain.lower())\n",
    "        w_subdomain = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", subdomain.lower())   \n",
    "        w_path = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", path.lower())\n",
    "        raw_words = w_domain + w_path + w_subdomain\n",
    "        w_host = w_domain + w_subdomain\n",
    "        raw_words = list(filter(None,raw_words))\n",
    "        return raw_words, list(filter(None,w_host)), list(filter(None,w_path))\n",
    "\n",
    "    \n",
    "    Href = {'internals':[], 'externals':[], 'null':[]}\n",
    "    Link = {'internals':[], 'externals':[], 'null':[]}\n",
    "    Anchor = {'safe':[], 'unsafe':[], 'null':[]}\n",
    "    Media = {'internals':[], 'externals':[], 'null':[]}\n",
    "    Form = {'internals':[], 'externals':[], 'null':[]}\n",
    "    CSS = {'internals':[], 'externals':[], 'null':[]}\n",
    "    Favicon = {'internals':[], 'externals':[], 'null':[]}\n",
    "    IFrame = {'visible':[], 'invisible':[], 'null':[]}\n",
    "    Title =''\n",
    "    Text= ''\n",
    "    state, iurl, page = accesibilidad_URL(url)\n",
    "    if state:\n",
    "        content = page.content\n",
    "        hostname, domain, path = sacar_dominio(url)\n",
    "        extracted_domain = tldextract.extract(url)\n",
    "        domain = extracted_domain.domain+'.'+extracted_domain.suffix\n",
    "        subdomain = extracted_domain.subdomain\n",
    "        tmp = url[url.find(extracted_domain.suffix):len(url)]\n",
    "        pth = tmp.partition(\"/\")\n",
    "        path = pth[1] + pth[2]\n",
    "        words_raw, words_raw_host, words_raw_path= extraccion_palabras(extracted_domain.domain, subdomain, pth[2])\n",
    "        tld = extracted_domain.suffix\n",
    "        parsed = urlparse(url)\n",
    "        scheme = parsed.scheme\n",
    "        \n",
    "        Href, Link, Anchor, Media, Form, CSS, Favicon, IFrame, Title, Text = extraccion_datos_URL(hostname, content, domain, Href, Link, Anchor, Media, Form, CSS, Favicon, IFrame, Title, Text)\n",
    "\n",
    "        row = [google_index(url),\n",
    "               page_rank(domain),\n",
    "               nb_hyperlinks(Href, Link, Media, Form, CSS, Favicon),\n",
    "               traffic_web(url),\n",
    "               check_www(words_raw),\n",
    "               domain_age(domain),\n",
    "               external_hyperlinks(Href, Link, Media, Form, CSS, Favicon),\n",
    "               longest_word_length(words_raw_path),\n",
    "               phish_hints(url),\n",
    "               internal_hyperlinks(Href, Link, Media, Form, CSS, Favicon),\n",
    "               safe_anchor(Anchor),\n",
    "               count_digits(domain),\n",
    "               longest_word_length(words_raw),\n",
    "               url_length(hostname),\n",
    "               links_in_tags(Link),\n",
    "               average_word_length(words_raw_path),\n",
    "               length_word_raw(words_raw),\n",
    "               char_repeat(words_raw),\n",
    "               domain_registration_length(domain),\n",
    "               external_redirection(Href, Link, Media, Form, CSS, Favicon),\n",
    "               shortest_word_length(words_raw_host),\n",
    "               ratio_digits(hostname),\n",
    "               count_slash(url),\n",
    "               shortest_word_length(words_raw_path),\n",
    "               domain_in_title(extracted_domain.domain, Title),\n",
    "               count_dots(hostname),\n",
    "               count_hyphens(url),\n",
    "               average_word_length(words_raw)]\n",
    "        return row\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " -1,\n",
       " 23,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " None,\n",
       " 0,\n",
       " 0,\n",
       " 0.08695652173913043,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 13,\n",
       " 8.333333333333332,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 0.0,\n",
       " 3,\n",
       " 0.0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4.5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fila = extraccion_parametros(url)\n",
    "fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fila)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
