{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import whois\n",
    "import time\n",
    "import re\n",
    "import urllib\n",
    "import tldextract\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.marca.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_URL_accessible(url):\n",
    "    page = None\n",
    "    try:\n",
    "        page = requests.get(url, timeout=5)   \n",
    "    except:\n",
    "        parsed = urlparse(url)\n",
    "        url = parsed.scheme+'://'+parsed.netloc\n",
    "        if not parsed.netloc.startswith('www'):\n",
    "            url = parsed.scheme+'://www.'+parsed.netloc\n",
    "            try:\n",
    "                page = requests.get(url, timeout=5)\n",
    "            except:\n",
    "                page = None\n",
    "                pass\n",
    "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
    "        return True, url, page\n",
    "    else:\n",
    "        return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominio: www.marca.com, Subdominio: marca, Top-Level Domain: \n"
     ]
    }
   ],
   "source": [
    "def get_domain(url):\n",
    "    o = urllib.parse.urlsplit(url)\n",
    "    return o.hostname, tldextract.extract(url).domain, o.path\n",
    "\n",
    "x = get_domain(url)\n",
    "\n",
    "dominio = ''.join(x)\n",
    "\n",
    "print(f'Dominio: {x[0]}, Subdominio: {x[1]}, Top-Level Domain: {x[2]}')\n",
    "\n",
    "domain = x[0]\n",
    "subdomain = x[1]\n",
    "Top_Level_Domain = x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.79M/6.79M [00:00<00:00, 7.19MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def non_google_index(url):\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    google = 'https://www.google.com/webhp?hl=es&sa=X&ved=0ahUKEwjR1qmah6f-AhU8VqQEHQKCCAcQPAgJ'\n",
    "    driver.get(google)\n",
    "\n",
    "    aceptar = driver.find_element(By.ID, \"L2AGLb\")\n",
    "    aceptar.click()\n",
    "    time.sleep(5)\n",
    "\n",
    "    buscar = driver.find_element(By.ID, 'APjFqb')\n",
    "    buscar.send_keys(f'site:{url}')\n",
    "    time.sleep(5)\n",
    "    buscar.send_keys(Keys.ENTER)\n",
    "\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        url_1 = driver.find_element(By.CLASS_NAME, \"qLRx3b.tjvcx.GvPZzd.cHaqb\")\n",
    "        url_text = url_1.text\n",
    "    except:\n",
    "        url_text = '0'\n",
    "\n",
    "    url_text = url_text[:(len(url) + 1)]\n",
    "\n",
    "    if url == url_text:\n",
    "        google_index = 0\n",
    "    else:\n",
    "        google_index = 1\n",
    "\n",
    "    return google_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#domain_registration_length\n",
    "def domain_registration_length(domain):\n",
    "    longitud_dominio = len(domain)\n",
    "    return longitud_dominio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#page_rank\n",
    "def web_traffic(url):\n",
    "        try:\n",
    "            rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + short_url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        except:\n",
    "            return 0\n",
    "        return int(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe_anchor\n",
    "def safe_anchor(Anchor):\n",
    "    total = len(Anchor['safe']) +  len(Anchor['unsafe'])\n",
    "    unsafe = len(Anchor['unsafe'])\n",
    "    try:\n",
    "        percentile = unsafe / float(total) * 100\n",
    "    except:\n",
    "        return 0\n",
    "    return percentile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_www\n",
    "def check_www(words_raw):\n",
    "        count = 0\n",
    "        for word in words_raw:\n",
    "            if not word.find('www') == -1:\n",
    "                count += 1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numero de hipervinculos totales\n",
    "\n",
    "def nb_hyperlinks(Href, Link, Media, Form, CSS, Favicon):\n",
    "    return len(Href['internals']) + len(Href['externals']) +\\\n",
    "            len(Link['internals']) + len(Link['externals']) +\\\n",
    "            len(Media['internals']) + len(Media['externals']) +\\\n",
    "            len(Form['internals']) + len(Form['externals']) +\\\n",
    "            len(CSS['internals']) + len(CSS['externals']) +\\\n",
    "            len(Favicon['internals']) + len(Favicon['externals'])\n",
    "\n",
    "#redirecciones totales\n",
    "\n",
    "def h_total(Href, Link, Media, Form, CSS, Favicon):\n",
    "    return nb_hyperlinks(Href, Link, Media, Form, CSS, Favicon)\n",
    "\n",
    "#ratio_extRedirection\n",
    "\n",
    "def h_external(Href, Link, Media, Form, CSS, Favicon):\n",
    "    return len(Href['externals']) + len(Link['externals']) + len(Media['externals']) +\\\n",
    "           len(Form['externals']) + len(CSS['externals']) + len(Favicon['externals'])\n",
    "\n",
    "def count_external_redirection(page, domain):\n",
    "    count = 0\n",
    "    if len(page.history) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        for i, response in enumerate(page.history,1):\n",
    "            if domain.lower() not in response.url.lower():\n",
    "                count+=1          \n",
    "            return count\n",
    "        \n",
    "def h_e_redirect(Href, Link, Media, Form, CSS, Favicon):\n",
    "    count = 0\n",
    "    for link in Href['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Link['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Media['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Media['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue \n",
    "    for link in Form['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue    \n",
    "    for link in CSS['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue    \n",
    "    for link in Favicon['externals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue    \n",
    "    return count\n",
    "\n",
    "def external_redirection(Href, Link, Media, Form, CSS, Favicon):\n",
    "    externals = h_external(Href, Link, Media, Form, CSS, Favicon)\n",
    "    if (externals>0):\n",
    "        return h_e_redirect(Href, Link, Media, Form, CSS, Favicon)/externals\n",
    "    return 0\n",
    "\n",
    "\n",
    "#ratio_intRedirection\n",
    "\n",
    "def h_internal(Href, Link, Media, Form, CSS, Favicon):\n",
    "    return len(Href['internals']) + len(Link['internals']) + len(Media['internals']) +\\\n",
    "           len(Form['internals']) + len(CSS['internals']) + len(Favicon['internals'])\n",
    "\n",
    "\n",
    "def h_i_redirect(Href, Link, Media, Form, CSS, Favicon):\n",
    "    count = 0\n",
    "    for link in Href['internals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Link['internals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Media['internals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Form['internals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in CSS['internals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    for link in Favicon['internals']:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if len(r.history) > 0:\n",
    "                count+=1\n",
    "        except:\n",
    "            continue\n",
    "    return count\n",
    "\n",
    "def internal_redirection(Href, Link, Media, Form, CSS, Favicon):\n",
    "    internals = h_internal(Href, Link, Media, Form, CSS, Favicon)\n",
    "    if (internals>0):\n",
    "        return h_i_redirect(Href, Link, Media, Form, CSS, Favicon)/internals\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_words_raw\n",
    "def average_word_length(words_raw):\n",
    "    if len(words_raw) ==0:\n",
    "        return 0\n",
    "    return sum(len(word) for word in words_raw) / len(words_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio_digits_url\n",
    "def count_digits(line):\n",
    "    return len(re.sub(\"[^0-9]\", \"\", line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_dots\n",
    "def count_dots(hostname):\n",
    "    return hostname.count('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio_digits_hostname\n",
    "def ratio_digits(hostname):\n",
    "    return len(re.sub(\"[^0-9]\", \"\", hostname))/len(hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#domain_age\n",
    "def domain_age(domain):\n",
    "\n",
    "    url = domain.split(\"//\")[-1].split(\"/\")[0].split('?')[0]\n",
    "    show = \"https://input.payapi.io/v1/api/fraud/domain/age/\" + url\n",
    "    r = requests.get(show)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        data = r.text\n",
    "        jsonToPython = json.loads(data)\n",
    "        result = jsonToPython['result']\n",
    "        if result == None:\n",
    "            return -2\n",
    "        else:\n",
    "            return result\n",
    "    else:       \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#domain_in_title\n",
    "def domain_in_title(domain, title):\n",
    "    if domain.lower() in title.lower(): \n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio_extHyperlinks\n",
    "def external_hyperlinks(Href, Link, Media, Form, CSS, Favicon):\n",
    "    total = h_total(Href, Link, Media, Form, CSS, Favicon)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    else :\n",
    "        return h_external(Href, Link, Media, Form, CSS, Favicon)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#longest_word_raw\n",
    "def longest_word_length(words_raw):\n",
    "    if len(words_raw) ==0:\n",
    "        return 0\n",
    "    return max(len(word) for word in words_raw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortest_word_host\n",
    "def shortest_word_length(words_raw_host):\n",
    "    if len(words_raw_host) ==0:\n",
    "        return 0\n",
    "    return min(len(word) for word in words_raw_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio_intHyperlinks\n",
    "def internal_hyperlinks(Href, Link, Media, Form, CSS, Favicon):\n",
    "    total = h_total(Href, Link, Media, Form, CSS, Favicon)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    else :\n",
    "        return h_internal(Href, Link, Media, Form, CSS, Favicon)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#longest_word_path\n",
    "def longest_word_length(words_raw_path):\n",
    "    if len(words_raw_path) ==0:\n",
    "        return 0\n",
    "    return max(len(word) for word in words_raw_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length_hostname\n",
    "def url_length(hostname):\n",
    "    return len(hostname) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_word_path\n",
    "def average_word_length(words_raw_path):\n",
    "    if len(words_raw_path) ==0:\n",
    "        return 0\n",
    "    return sum(len(word) for word in words_raw_path) / len(words_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length_words_raw\n",
    "def length_word_raw(words_raw):\n",
    "    return len(words_raw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phish_hints\n",
    "HINTS = ['wp', 'login', 'includes', 'admin', 'content', 'site', 'images', 'js', 'alibaba', 'css', 'myaccount', 'dropbox', 'themes', 'plugins', 'signin', 'view']\n",
    "def phish_hints(url_path):\n",
    "    count = 0\n",
    "    for hint in HINTS:\n",
    "        count += url_path.lower().count(hint)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortest_word_path\n",
    "def shortest_word_length(words_raw_path):\n",
    "    if len(words_raw_path) ==0:\n",
    "        return 0\n",
    "    return min(len(word) for word in words_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_hyphens\n",
    "def count_hyphens(base_url):\n",
    "    return base_url.count('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#web_traffic\n",
    "def web_traffic(short_url):\n",
    "        try:\n",
    "            rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + short_url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        except:\n",
    "            return 0\n",
    "        return int(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#char_repeat\n",
    "def char_repeat(words_raw):\n",
    "    \n",
    "        def __all_same(items):\n",
    "            return all(x == items[0] for x in items)\n",
    "\n",
    "        repeat = {'2': 0, '3': 0, '4': 0, '5': 0}\n",
    "        part = [2, 3, 4, 5]\n",
    "\n",
    "        for word in words_raw:\n",
    "            for char_repeat_count in part:\n",
    "                for i in range(len(word) - char_repeat_count + 1):\n",
    "                    sub_word = word[i:i + char_repeat_count]\n",
    "                    if __all_same(sub_word):\n",
    "                        repeat[str(char_repeat_count)] = repeat[str(char_repeat_count)] + 1\n",
    "        return  sum(list(repeat.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#links_in_tags\n",
    "def links_in_tags(Link):\n",
    "    total = len(Link['internals']) +  len(Link['externals'])\n",
    "    internals = len(Link['internals'])\n",
    "    try:\n",
    "        percentile = internals / float(total) * 100\n",
    "    except:\n",
    "        return 0\n",
    "    return percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_slash\n",
    "def count_slash(full_url):\n",
    "    return full_url.count('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.marca.com', 1, ('www.marca.com', 'marca', ''), (1, -1), 0, 72.05882352941177, 1, 0.05228758169934641, 4.0, 0, 2, 0.0, 0, 0.95625, 5, 3, 160, 0.04375, 0, 13, 0, 2, 0, 0, 0, 0, 3, 7.4074074074074066, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://www.marca.com',\n",
       " 1,\n",
       " ('www.marca.com', 'marca', ''),\n",
       " (1, -1),\n",
       " 0,\n",
       " 72.05882352941177,\n",
       " 1,\n",
       " 0.05228758169934641,\n",
       " 4.0,\n",
       " 0,\n",
       " 2,\n",
       " 0.0,\n",
       " 0,\n",
       " 0.95625,\n",
       " 5,\n",
       " 3,\n",
       " 160,\n",
       " 0.04375,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 7.4074074074074066,\n",
       " 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_data_from_URL(hostname, content, domain, Href, Link, Anchor, Media, Form, CSS, Favicon, IFrame, Title, Text):\n",
    "    Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
    "\n",
    "    # collect all external and internal hrefs from url\n",
    "    for href in soup.find_all('a', href=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', href['href'])]\n",
    "        if hostname in href['href'] or domain in href['href'] or len(dots) == 1 or not href['href'].startswith('http'):\n",
    "            if \"#\" in href['href'] or \"javascript\" in href['href'].lower() or \"mailto\" in href['href'].lower():\n",
    "                 Anchor['unsafe'].append(href['href']) \n",
    "            if not href['href'].startswith('http'):\n",
    "                if not href['href'].startswith('/'):\n",
    "                    Href['internals'].append(hostname+'/'+href['href']) \n",
    "                elif href['href'] in Null_format:\n",
    "                    Href['null'].append(href['href'])  \n",
    "                else:\n",
    "                    Href['internals'].append(hostname+href['href'])   \n",
    "        else:\n",
    "            Href['externals'].append(href['href'])\n",
    "            Anchor['safe'].append(href['href'])\n",
    "\n",
    "    # collect all media src tags\n",
    "    for img in soup.find_all('img', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
    "        if hostname in img['src'] or domain in img['src'] or len(dots) == 1 or not img['src'].startswith('http'):\n",
    "            if not img['src'].startswith('http'):\n",
    "                if not img['src'].startswith('/'):\n",
    "                    Media['internals'].append(hostname+'/'+img['src']) \n",
    "                elif img['src'] in Null_format:\n",
    "                    Media['null'].append(img['src'])  \n",
    "                else:\n",
    "                    Media['internals'].append(hostname+img['src'])   \n",
    "        else:\n",
    "            Media['externals'].append(img['src'])\n",
    "           \n",
    "    \n",
    "    for audio in soup.find_all('audio', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
    "        if hostname in audio['src'] or domain in audio['src'] or len(dots) == 1 or not audio['src'].startswith('http'):\n",
    "             if not audio['src'].startswith('http'):\n",
    "                if not audio['src'].startswith('/'):\n",
    "                    Media['internals'].append(hostname+'/'+audio['src']) \n",
    "                elif audio['src'] in Null_format:\n",
    "                    Media['null'].append(audio['src'])  \n",
    "                else:\n",
    "                    Media['internals'].append(hostname+audio['src'])   \n",
    "        else:\n",
    "            Media['externals'].append(audio['src'])\n",
    "            \n",
    "    for embed in soup.find_all('embed', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', embed['src'])]\n",
    "        if hostname in embed['src'] or domain in embed['src'] or len(dots) == 1 or not embed['src'].startswith('http'):\n",
    "             if not embed['src'].startswith('http'):\n",
    "                if not embed['src'].startswith('/'):\n",
    "                    Media['internals'].append(hostname+'/'+embed['src']) \n",
    "                elif embed['src'] in Null_format:\n",
    "                    Media['null'].append(embed['src'])  \n",
    "                else:\n",
    "                    Media['internals'].append(hostname+embed['src'])   \n",
    "        else:\n",
    "            Media['externals'].append(embed['src'])\n",
    "           \n",
    "    for i_frame in soup.find_all('iframe', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', i_frame['src'])]\n",
    "        if hostname in i_frame['src'] or domain in i_frame['src'] or len(dots) == 1 or not i_frame['src'].startswith('http'):\n",
    "            if not i_frame['src'].startswith('http'):\n",
    "                if not i_frame['src'].startswith('/'):\n",
    "                    Media['internals'].append(hostname+'/'+i_frame['src']) \n",
    "                elif i_frame['src'] in Null_format:\n",
    "                    Media['null'].append(i_frame['src'])  \n",
    "                else:\n",
    "                    Media['internals'].append(hostname+i_frame['src'])   \n",
    "        else: \n",
    "            Media['externals'].append(i_frame['src'])\n",
    "           \n",
    "\n",
    "    # collect all link tags\n",
    "    for link in soup.findAll('link', href=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', link['href'])]\n",
    "        if hostname in link['href'] or domain in link['href'] or len(dots) == 1 or not link['href'].startswith('http'):\n",
    "            if not link['href'].startswith('http'):\n",
    "                if not link['href'].startswith('/'):\n",
    "                    Link['internals'].append(hostname+'/'+link['href']) \n",
    "                elif link['href'] in Null_format:\n",
    "                    Link['null'].append(link['href'])  \n",
    "                else:\n",
    "                    Link['internals'].append(hostname+link['href'])   \n",
    "        else:\n",
    "            Link['externals'].append(link['href'])\n",
    "\n",
    "    for script in soup.find_all('script', src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', script['src'])]\n",
    "        if hostname in script['src'] or domain in script['src'] or len(dots) == 1 or not script['src'].startswith('http'):\n",
    "            if not script['src'].startswith('http'):\n",
    "                if not script['src'].startswith('/'):\n",
    "                    Link['internals'].append(hostname+'/'+script['src']) \n",
    "                elif script['src'] in Null_format:\n",
    "                    Link['null'].append(script['src'])  \n",
    "                else:\n",
    "                    Link['internals'].append(hostname+script['src'])   \n",
    "        else:\n",
    "            Link['externals'].append(link['href'])\n",
    "           \n",
    "            \n",
    "    # collect all css\n",
    "    for link in soup.find_all('link', rel='stylesheet'):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', link['href'])]\n",
    "        if hostname in link['href'] or domain in link['href'] or len(dots) == 1 or not link['href'].startswith('http'):\n",
    "            if not link['href'].startswith('http'):\n",
    "                if not link['href'].startswith('/'):\n",
    "                    CSS['internals'].append(hostname+'/'+link['href']) \n",
    "                elif link['href'] in Null_format:\n",
    "                    CSS['null'].append(link['href'])  \n",
    "                else:\n",
    "                    CSS['internals'].append(hostname+link['href'])   \n",
    "        else:\n",
    "            CSS['externals'].append(link['href'])\n",
    "    \n",
    "    for style in soup.find_all('style', type='text/css'):\n",
    "        try: \n",
    "            start = str(style[0]).index('@import url(')\n",
    "            end = str(style[0]).index(')')\n",
    "            css = str(style[0])[start+12:end]\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', css)]\n",
    "            if hostname in css or domain in css or len(dots) == 1 or not css.startswith('http'):\n",
    "                if not css.startswith('http'):\n",
    "                    if not css.startswith('/'):\n",
    "                        CSS['internals'].append(hostname+'/'+css) \n",
    "                    elif css in Null_format:\n",
    "                        CSS['null'].append(css)  \n",
    "                    else:\n",
    "                        CSS['internals'].append(hostname+css)   \n",
    "            else: \n",
    "                CSS['externals'].append(css)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # collect all form actions\n",
    "    for form in soup.findAll('form', action=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.', form['action'])]\n",
    "        if hostname in form['action'] or domain in form['action'] or len(dots) == 1 or not form['action'].startswith('http'):\n",
    "            if not form['action'].startswith('http'):\n",
    "                if not form['action'].startswith('/'):\n",
    "                    Form['internals'].append(hostname+'/'+form['action']) \n",
    "                elif form['action'] in Null_format or form['action'] == 'about:blank':\n",
    "                    Form['null'].append(form['action'])  \n",
    "                else:\n",
    "                    Form['internals'].append(hostname+form['action'])   \n",
    "        else:\n",
    "            Form['externals'].append(form['action'])\n",
    "            \n",
    "\n",
    "    # collect all link tags\n",
    "    for head in soup.find_all('head'):\n",
    "        for head.link in soup.find_all('link', href=True):\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
    "            if hostname in head.link['href'] or len(dots) == 1 or domain in head.link['href'] or not head.link['href'].startswith('http'):\n",
    "                if not head.link['href'].startswith('http'):\n",
    "                    if not head.link['href'].startswith('/'):\n",
    "                        Favicon['internals'].append(hostname+'/'+head.link['href']) \n",
    "                    elif head.link['href'] in Null_format:\n",
    "                        Favicon['null'].append(head.link['href'])  \n",
    "                    else:\n",
    "                        Favicon['internals'].append(hostname+head.link['href'])   \n",
    "            else:\n",
    "                Favicon['externals'].append(head.link['href'])\n",
    "                \n",
    "        for head.link in soup.findAll('link', {'href': True, 'rel':True}):\n",
    "            isicon = False\n",
    "            if isinstance(head.link['rel'], list):\n",
    "                for e_rel in head.link['rel']:\n",
    "                    if (e_rel.endswith('icon')):\n",
    "                        isicon = True\n",
    "            else:\n",
    "                if (head.link['rel'].endswith('icon')):\n",
    "                    isicon = True\n",
    "       \n",
    "            if isicon:\n",
    "                 dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
    "                 if hostname in head.link['href'] or len(dots) == 1 or domain in head.link['href'] or not head.link['href'].startswith('http'):\n",
    "                     if not head.link['href'].startswith('http'):\n",
    "                        if not head.link['href'].startswith('/'):\n",
    "                            Favicon['internals'].append(hostname+'/'+head.link['href']) \n",
    "                        elif head.link['href'] in Null_format:\n",
    "                            Favicon['null'].append(head.link['href'])  \n",
    "                        else:\n",
    "                            Favicon['internals'].append(hostname+head.link['href'])   \n",
    "                 else:\n",
    "                     Favicon['externals'].append(head.link['href'])\n",
    "                     \n",
    "                    \n",
    "    # collect i_frame\n",
    "    for i_frame in soup.find_all('iframe', width=True, height=True, frameborder=True):\n",
    "        if i_frame['width'] == \"0\" and i_frame['height'] == \"0\" and i_frame['frameborder'] == \"0\":\n",
    "            IFrame['invisible'].append(i_frame)\n",
    "        else:\n",
    "            IFrame['visible'].append(i_frame)\n",
    "    for i_frame in soup.find_all('iframe', width=True, height=True, border=True):\n",
    "        if i_frame['width'] == \"0\" and i_frame['height'] == \"0\" and i_frame['border'] == \"0\":\n",
    "            IFrame['invisible'].append(i_frame)\n",
    "        else:\n",
    "            IFrame['visible'].append(i_frame)\n",
    "    for i_frame in soup.find_all('iframe', width=True, height=True, style=True):\n",
    "        if i_frame['width'] == \"0\" and i_frame['height'] == \"0\" and i_frame['style'] == \"border:none;\":\n",
    "            IFrame['invisible'].append(i_frame)\n",
    "        else:\n",
    "            IFrame['visible'].append(i_frame)\n",
    "          \n",
    "    # get page title\n",
    "    try:\n",
    "        Title = soup.title.string\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # get content text\n",
    "    Text = soup.get_text()\n",
    "    \n",
    "    return Href, Link, Anchor, Media, Form, CSS, Favicon, IFrame, Title, Text\n",
    "\n",
    "def extract_features(url):\n",
    "    \n",
    "    def words_raw_extraction(domain, subdomain, path):\n",
    "        w_domain = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", domain.lower())\n",
    "        w_subdomain = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", subdomain.lower())   \n",
    "        w_path = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", path.lower())\n",
    "        raw_words = w_domain + w_path + w_subdomain\n",
    "        w_host = w_domain + w_subdomain\n",
    "        raw_words = list(filter(None,raw_words))\n",
    "        return raw_words, list(filter(None,w_host)), list(filter(None,w_path))\n",
    "\n",
    "    \n",
    "    Href = {'internals':[], 'externals':[], 'null':[]}\n",
    "    Link = {'internals':[], 'externals':[], 'null':[]}\n",
    "    Anchor = {'safe':[], 'unsafe':[], 'null':[]}\n",
    "    Media = {'internals':[], 'externals':[], 'null':[]}\n",
    "    Form = {'internals':[], 'externals':[], 'null':[]}\n",
    "    CSS = {'internals':[], 'externals':[], 'null':[]}\n",
    "    Favicon = {'internals':[], 'externals':[], 'null':[]}\n",
    "    IFrame = {'visible':[], 'invisible':[], 'null':[]}\n",
    "    Title =''\n",
    "    Text= ''\n",
    "    state, iurl, page = is_URL_accessible(url)\n",
    "    if state:\n",
    "        content = page.content\n",
    "        hostname, domain, path = get_domain(url)\n",
    "        extracted_domain = tldextract.extract(url)\n",
    "        domain = extracted_domain.domain+'.'+extracted_domain.suffix\n",
    "        subdomain = extracted_domain.subdomain\n",
    "        tmp = url[url.find(extracted_domain.suffix):len(url)]\n",
    "        pth = tmp.partition(\"/\")\n",
    "        path = pth[1] + pth[2]\n",
    "        words_raw, words_raw_host, words_raw_path= words_raw_extraction(extracted_domain.domain, subdomain, pth[2])\n",
    "        tld = extracted_domain.suffix\n",
    "        parsed = urlparse(url)\n",
    "        scheme = parsed.scheme\n",
    "        \n",
    "        Href, Link, Anchor, Media, Form, CSS, Favicon, IFrame, Title, Text = extract_data_from_URL(hostname, content, domain, Href, Link, Anchor, Media, Form, CSS, Favicon, IFrame, Title, Text)\n",
    "\n",
    "        row = [url,\n",
    "               non_google_index(url),\n",
    "               get_domain(url),\n",
    "               domain_registration_length(domain),\n",
    "               web_traffic(url),\n",
    "               safe_anchor(Anchor),\n",
    "               check_www(words_raw),\n",
    "               external_redirection(Href, Link, Media, Form, CSS, Favicon),\n",
    "               average_word_length(words_raw),\n",
    "               count_digits(dominio),\n",
    "               count_dots(hostname),\n",
    "               ratio_digits(hostname),\n",
    "               \n",
    "               domain_in_title(extracted_domain.domain, Title),\n",
    "               external_hyperlinks(Href, Link, Media, Form, CSS, Favicon),\n",
    "               longest_word_length(words_raw),\n",
    "               shortest_word_length(words_raw_host),\n",
    "               nb_hyperlinks(Href, Link, Media, Form, CSS, Favicon),\n",
    "               internal_hyperlinks(Href, Link, Media, Form, CSS, Favicon),\n",
    "               longest_word_length(words_raw_path),\n",
    "               url_length(hostname),\n",
    "               average_word_length(words_raw_path),\n",
    "               length_word_raw(words_raw),\n",
    "               phish_hints(url),\n",
    "               shortest_word_length(words_raw_path),\n",
    "               count_hyphens(url),\n",
    "               web_traffic(url),\n",
    "               char_repeat(words_raw),\n",
    "               links_in_tags(Link),\n",
    "               count_slash(url)]\n",
    "        print(row)\n",
    "        return row\n",
    "    return None\n",
    "\n",
    "extract_features(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
